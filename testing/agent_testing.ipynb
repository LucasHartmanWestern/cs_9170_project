{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from dqn_agent import DQNAgent\n",
    "from ppo_agent import PPOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the agents\n",
    "\n",
    "# Environment parameters (example: CartPole)\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = 2\n",
    "HIDDEN_SIZE = 64 # number of neurons in hidden layers\n",
    "LEARNING_RATE = 1e-3 # learning rate\n",
    "GAMMA = 0.99 # discount factor\n",
    "BATCH_SIZE = 32 # mini-batch size for training\n",
    "MEMORY_SIZE = 10000 # replay buffer size\n",
    "EPSILON_START = 1.0 # starting epsilon for exploration\n",
    "EPSILON_MIN = 0.01 # minimum epsilon value\n",
    "EPSILON_DECAY = 0.995 # decay rate for epsilon\n",
    "CLIP_EPSILON = 0.2 # clipping parameter for PPO objective\n",
    "UPDATE_EPOCHS = 4 # number of epochs to update the policy per training iteration\n",
    "C1 = 0.5 # coefficient for the value loss\n",
    "C2 = 0.01 # coefficient for the entropy bonus\n",
    "\n",
    "# ================================\n",
    "# DQN Agent Parameters\n",
    "# ================================\n",
    "DQN_PARAMS = {\n",
    "    'state_size': STATE_SIZE,\n",
    "    'action_size': ACTION_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,         \n",
    "    'lr': LEARNING_RATE,\n",
    "    'gamma': GAMMA,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'memory_size': MEMORY_SIZE,\n",
    "    'epsilon_start': EPSILON_START,\n",
    "    'epsilon_min': EPSILON_MIN,\n",
    "    'epsilon_decay': EPSILON_DECAY\n",
    "}\n",
    "\n",
    "# ================================\n",
    "# PPO Agent Parameters\n",
    "# ================================\n",
    "PPO_PARAMS = {\n",
    "    'state_size': STATE_SIZE,\n",
    "    'action_size': ACTION_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'lr': LEARNING_RATE,\n",
    "    'gamma': GAMMA,\n",
    "    'clip_epsilon': CLIP_EPSILON,\n",
    "    'update_epochs': UPDATE_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'c1': C1,\n",
    "    'c2': C2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(**DQN_PARAMS)\n",
    "ppo_agent = PPOAgent(**PPO_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Monkey patch: alias np.bool8 to np.bool_ if it's missing.\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "import gym\n",
    "\n",
    "# Create the CartPole environment (or replace with your desired environment)\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "print(\"Environment loaded:\", env.spec.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN Agent:\n",
      "DQN Episode 1: Total Reward = 17.0\n",
      "DQN Episode 2: Total Reward = 14.0\n",
      "DQN Episode 3: Total Reward = 17.0\n",
      "DQN Episode 4: Total Reward = 38.0\n",
      "DQN Episode 5: Total Reward = 18.0\n",
      "DQN Episode 6: Total Reward = 20.0\n",
      "DQN Episode 7: Total Reward = 13.0\n",
      "DQN Episode 8: Total Reward = 19.0\n",
      "DQN Episode 9: Total Reward = 11.0\n",
      "DQN Episode 10: Total Reward = 11.0\n",
      "DQN Episode 11: Total Reward = 17.0\n",
      "DQN Episode 12: Total Reward = 10.0\n",
      "DQN Episode 13: Total Reward = 20.0\n",
      "DQN Episode 14: Total Reward = 17.0\n",
      "DQN Episode 15: Total Reward = 9.0\n",
      "DQN Episode 16: Total Reward = 10.0\n",
      "DQN Episode 17: Total Reward = 10.0\n",
      "DQN Episode 18: Total Reward = 9.0\n",
      "DQN Episode 19: Total Reward = 11.0\n",
      "DQN Episode 20: Total Reward = 12.0\n",
      "DQN Episode 21: Total Reward = 11.0\n",
      "DQN Episode 22: Total Reward = 16.0\n",
      "DQN Episode 23: Total Reward = 29.0\n",
      "DQN Episode 24: Total Reward = 16.0\n",
      "DQN Episode 25: Total Reward = 30.0\n",
      "DQN Episode 26: Total Reward = 77.0\n",
      "DQN Episode 27: Total Reward = 78.0\n",
      "DQN Episode 28: Total Reward = 49.0\n",
      "DQN Episode 29: Total Reward = 49.0\n",
      "DQN Episode 30: Total Reward = 42.0\n",
      "DQN Episode 31: Total Reward = 34.0\n",
      "DQN Episode 32: Total Reward = 76.0\n",
      "DQN Episode 33: Total Reward = 59.0\n",
      "DQN Episode 34: Total Reward = 55.0\n",
      "DQN Episode 35: Total Reward = 27.0\n",
      "DQN Episode 36: Total Reward = 39.0\n",
      "DQN Episode 37: Total Reward = 75.0\n",
      "DQN Episode 38: Total Reward = 29.0\n",
      "DQN Episode 39: Total Reward = 59.0\n",
      "DQN Episode 40: Total Reward = 106.0\n",
      "DQN Episode 41: Total Reward = 41.0\n",
      "DQN Episode 42: Total Reward = 45.0\n",
      "DQN Episode 43: Total Reward = 95.0\n",
      "DQN Episode 44: Total Reward = 41.0\n",
      "DQN Episode 45: Total Reward = 101.0\n",
      "DQN Episode 46: Total Reward = 69.0\n",
      "DQN Episode 47: Total Reward = 138.0\n",
      "DQN Episode 48: Total Reward = 131.0\n",
      "DQN Episode 49: Total Reward = 208.0\n",
      "DQN Episode 50: Total Reward = 99.0\n",
      "DQN Episode 51: Total Reward = 184.0\n",
      "DQN Episode 52: Total Reward = 211.0\n",
      "DQN Episode 53: Total Reward = 96.0\n",
      "DQN Episode 54: Total Reward = 500.0\n",
      "DQN Episode 55: Total Reward = 319.0\n",
      "DQN Episode 56: Total Reward = 215.0\n",
      "DQN Episode 57: Total Reward = 240.0\n",
      "DQN Episode 58: Total Reward = 500.0\n",
      "DQN Episode 59: Total Reward = 280.0\n",
      "DQN Episode 60: Total Reward = 233.0\n",
      "DQN Episode 61: Total Reward = 268.0\n",
      "DQN Episode 62: Total Reward = 234.0\n",
      "DQN Episode 63: Total Reward = 500.0\n",
      "DQN Episode 64: Total Reward = 500.0\n",
      "DQN Episode 65: Total Reward = 500.0\n",
      "DQN Episode 66: Total Reward = 500.0\n",
      "DQN Episode 67: Total Reward = 500.0\n",
      "DQN Episode 68: Total Reward = 490.0\n",
      "DQN Episode 69: Total Reward = 500.0\n",
      "DQN Episode 70: Total Reward = 446.0\n",
      "DQN Episode 71: Total Reward = 376.0\n",
      "DQN Episode 72: Total Reward = 500.0\n",
      "DQN Episode 73: Total Reward = 417.0\n",
      "DQN Episode 74: Total Reward = 500.0\n",
      "DQN Episode 75: Total Reward = 500.0\n",
      "DQN Episode 76: Total Reward = 500.0\n",
      "DQN Episode 77: Total Reward = 357.0\n",
      "DQN Episode 78: Total Reward = 500.0\n",
      "DQN Episode 79: Total Reward = 500.0\n",
      "DQN Episode 80: Total Reward = 500.0\n",
      "DQN Episode 81: Total Reward = 500.0\n",
      "DQN Episode 82: Total Reward = 500.0\n",
      "DQN Episode 83: Total Reward = 500.0\n",
      "DQN Episode 84: Total Reward = 500.0\n",
      "DQN Episode 85: Total Reward = 500.0\n",
      "DQN Episode 86: Total Reward = 500.0\n",
      "DQN Episode 87: Total Reward = 500.0\n",
      "DQN Episode 88: Total Reward = 445.0\n",
      "DQN Episode 89: Total Reward = 500.0\n",
      "DQN Episode 90: Total Reward = 253.0\n",
      "DQN Episode 91: Total Reward = 336.0\n",
      "DQN Episode 92: Total Reward = 389.0\n",
      "DQN Episode 93: Total Reward = 495.0\n",
      "DQN Episode 94: Total Reward = 500.0\n",
      "DQN Episode 95: Total Reward = 500.0\n",
      "DQN Episode 96: Total Reward = 435.0\n",
      "DQN Episode 97: Total Reward = 500.0\n",
      "DQN Episode 98: Total Reward = 500.0\n",
      "DQN Episode 99: Total Reward = 500.0\n",
      "DQN Episode 100: Total Reward = 500.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "print(\"Training DQN Agent:\")\n",
    "for episode in range(num_episodes):\n",
    "    # Extract the observation from the reset tuple.\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = dqn_agent.predict(state)\n",
    "        # In Gym 0.26+, step returns (observation, reward, terminated, truncated, info)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # Store only the observation (state) in memory.\n",
    "        dqn_agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        dqn_agent.train()\n",
    "        \n",
    "    print(f\"DQN Episode {episode+1}: Total Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training PPO Agent:\n",
      "PPO Episode 1: Total Reward = 18.0\n",
      "PPO Episode 2: Total Reward = 13.0\n",
      "PPO Episode 3: Total Reward = 32.0\n",
      "PPO Episode 4: Total Reward = 20.0\n",
      "PPO Episode 5: Total Reward = 28.0\n",
      "PPO Episode 6: Total Reward = 16.0\n",
      "PPO Episode 7: Total Reward = 13.0\n",
      "PPO Episode 8: Total Reward = 16.0\n",
      "PPO Episode 9: Total Reward = 32.0\n",
      "PPO Episode 10: Total Reward = 32.0\n",
      "PPO Episode 11: Total Reward = 18.0\n",
      "PPO Episode 12: Total Reward = 13.0\n",
      "PPO Episode 13: Total Reward = 9.0\n",
      "PPO Episode 14: Total Reward = 35.0\n",
      "PPO Episode 15: Total Reward = 15.0\n",
      "PPO Episode 16: Total Reward = 14.0\n",
      "PPO Episode 17: Total Reward = 18.0\n",
      "PPO Episode 18: Total Reward = 17.0\n",
      "PPO Episode 19: Total Reward = 19.0\n",
      "PPO Episode 20: Total Reward = 19.0\n",
      "PPO Episode 21: Total Reward = 17.0\n",
      "PPO Episode 22: Total Reward = 71.0\n",
      "PPO Episode 23: Total Reward = 19.0\n",
      "PPO Episode 24: Total Reward = 15.0\n",
      "PPO Episode 25: Total Reward = 17.0\n",
      "PPO Episode 26: Total Reward = 13.0\n",
      "PPO Episode 27: Total Reward = 12.0\n",
      "PPO Episode 28: Total Reward = 52.0\n",
      "PPO Episode 29: Total Reward = 16.0\n",
      "PPO Episode 30: Total Reward = 22.0\n",
      "PPO Episode 31: Total Reward = 20.0\n",
      "PPO Episode 32: Total Reward = 15.0\n",
      "PPO Episode 33: Total Reward = 23.0\n",
      "PPO Episode 34: Total Reward = 43.0\n",
      "PPO Episode 35: Total Reward = 19.0\n",
      "PPO Episode 36: Total Reward = 19.0\n",
      "PPO Episode 37: Total Reward = 44.0\n",
      "PPO Episode 38: Total Reward = 16.0\n",
      "PPO Episode 39: Total Reward = 20.0\n",
      "PPO Episode 40: Total Reward = 20.0\n",
      "PPO Episode 41: Total Reward = 15.0\n",
      "PPO Episode 42: Total Reward = 33.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\New_OS_11\\OneDrive\\Documents\\GitHub\\cs_9170_project\\testing\\..\\ppo_agent.py:169: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  # Total loss with entropy bonus.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Episode 43: Total Reward = 60.0\n",
      "PPO Episode 44: Total Reward = 21.0\n",
      "PPO Episode 45: Total Reward = 22.0\n",
      "PPO Episode 46: Total Reward = 29.0\n",
      "PPO Episode 47: Total Reward = 25.0\n",
      "PPO Episode 48: Total Reward = 12.0\n",
      "PPO Episode 49: Total Reward = 19.0\n",
      "PPO Episode 50: Total Reward = 16.0\n",
      "PPO Episode 51: Total Reward = 11.0\n",
      "PPO Episode 52: Total Reward = 17.0\n",
      "PPO Episode 53: Total Reward = 19.0\n",
      "PPO Episode 54: Total Reward = 20.0\n",
      "PPO Episode 55: Total Reward = 14.0\n",
      "PPO Episode 56: Total Reward = 17.0\n",
      "PPO Episode 57: Total Reward = 12.0\n",
      "PPO Episode 58: Total Reward = 13.0\n",
      "PPO Episode 59: Total Reward = 44.0\n",
      "PPO Episode 60: Total Reward = 20.0\n",
      "PPO Episode 61: Total Reward = 24.0\n",
      "PPO Episode 62: Total Reward = 17.0\n",
      "PPO Episode 63: Total Reward = 10.0\n",
      "PPO Episode 64: Total Reward = 10.0\n",
      "PPO Episode 65: Total Reward = 12.0\n",
      "PPO Episode 66: Total Reward = 39.0\n",
      "PPO Episode 67: Total Reward = 35.0\n",
      "PPO Episode 68: Total Reward = 17.0\n",
      "PPO Episode 69: Total Reward = 11.0\n",
      "PPO Episode 70: Total Reward = 14.0\n",
      "PPO Episode 71: Total Reward = 27.0\n",
      "PPO Episode 72: Total Reward = 28.0\n",
      "PPO Episode 73: Total Reward = 13.0\n",
      "PPO Episode 74: Total Reward = 54.0\n",
      "PPO Episode 75: Total Reward = 53.0\n",
      "PPO Episode 76: Total Reward = 17.0\n",
      "PPO Episode 77: Total Reward = 33.0\n",
      "PPO Episode 78: Total Reward = 46.0\n",
      "PPO Episode 79: Total Reward = 18.0\n",
      "PPO Episode 80: Total Reward = 12.0\n",
      "PPO Episode 81: Total Reward = 14.0\n",
      "PPO Episode 82: Total Reward = 19.0\n",
      "PPO Episode 83: Total Reward = 73.0\n",
      "PPO Episode 84: Total Reward = 51.0\n",
      "PPO Episode 85: Total Reward = 57.0\n",
      "PPO Episode 86: Total Reward = 74.0\n",
      "PPO Episode 87: Total Reward = 29.0\n",
      "PPO Episode 88: Total Reward = 27.0\n",
      "PPO Episode 89: Total Reward = 15.0\n",
      "PPO Episode 90: Total Reward = 31.0\n",
      "PPO Episode 91: Total Reward = 20.0\n",
      "PPO Episode 92: Total Reward = 27.0\n",
      "PPO Episode 93: Total Reward = 28.0\n",
      "PPO Episode 94: Total Reward = 24.0\n",
      "PPO Episode 95: Total Reward = 56.0\n",
      "PPO Episode 96: Total Reward = 42.0\n",
      "PPO Episode 97: Total Reward = 22.0\n",
      "PPO Episode 98: Total Reward = 22.0\n",
      "PPO Episode 99: Total Reward = 60.0\n",
      "PPO Episode 100: Total Reward = 51.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining PPO Agent:\")\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # Get only the observation.\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = ppo_agent.predict(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # Store the transition (make sure only observations are stored).\n",
    "        ppo_agent.store_transition(state, ppo_agent.last_action, ppo_agent.last_log_prob, reward, done, ppo_agent.last_value)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    ppo_agent.train()\n",
    "    print(f\"PPO Episode {episode+1}: Total Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DQN Agent...\n",
      "Total evaluation reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating DQN Agent...\")\n",
    "\n",
    "# Reset the environment and extract the observation.\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Get action from the trained agent.\n",
    "    action = dqn_agent.predict(state)\n",
    "    # Step in the environment (Gym 0.26+ returns 5 values).\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(\"Total evaluation reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PPO Agent...\n",
      "Total evaluation reward: 51.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating PPO Agent...\")\n",
    "\n",
    "# Reset the environment and extract the observation.\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Get action from the trained agent.\n",
    "    action = ppo_agent.predict(state)\n",
    "    # Step in the environment (Gym 0.26+ returns 5 values).\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(\"Total evaluation reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.save(\"../saved_models/dqn_agent.pth\")\n",
    "ppo_agent.save(\"../saved_models/ppo_agent.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
